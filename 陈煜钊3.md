# Emotion recognition in conversation(ERC)

2016:

2017:

2018

2019:

1. $Context-aware\;Embedding\;for\;Targeted\;Aspect-based\;Sentiment\;Analysis$$\textcolor{Red}{(ACL,\;CCF-A)}$

   $\bullet\;Motivation:$

      本文是针对TABSA中，现有的基于注意力机制的神经网络模型虽然取得了较好的效果，但是由于这些方法中表示target和aspect的向量都是随机初始化的，导致在表示目标（target）和方面（aspect）时往往会脱离上下文。

   $\bullet\;Method:$

      本文提出了一种结合上下文信息优化目标和方面向量表示的方法，该方法可以直接和现有基于神经网络的目标-方面级别情感分析模型相结合。能够通过稀疏系数向量从上下文中选择一组高度相关的词，然后调整目标和方面的表示。因此，可以提取特定目标、对应方面和上下文之间的相互依赖关系，从而生成更好的嵌入。

   $\bullet\;Experimental\;Result:$

      结果显示本文提出的优化目标和方面向量表示的方法在目标识别和情感分类任务中都取得了更好的表现，在SentiHood和 Semeval两个基准数据集上均取得了更加优秀的成绩，这说明了上下文相关的目标和方面表示能提升模型在细粒度情感分析任务中的效果。**(Written by Yuzhao Chen)**   [(BibTex)](https://arxiv.org/abs/1906.06945)

2020:

1. $Context-Guided\;BERT\;for\;Targeted\;Aspect-Based\;Sentiment\;Analysis$$\textcolor{Red}{(AAAI,\;CCF-A)}$

   $\bullet\;Motivation:$

      在本文中，作者研究了在自我注意模型中加入上下文是否能提高(T)ABSA的表现。

   $\bullet\;Method:$

      本文提出了两种上下文引导的BERT(CGBERT)的变体，首先是采用上下文感知的Transformer来生成使用上下文引导的softmax-attention的CG-BERT，将之前提出的上下文感知自注意网络扩展到(T)ABSA任务。然后，提出了一种改进的准注意CG-BERT模型(QACG-BERT)，该模型可以学习支持减法注意的合成注意。

   $\bullet\;Experimental\;Result:$

      在SentiHood和SemEval数据集上，两种模型都取得了更好的表现，两种模型的性能均优于以前的SOTA模型。**(Written by Yuzhao Chen)**    [(BibTex)](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Context-Guided+BERT+for+Targeted+Aspect-Based+Sentiment+Analysis&btnG=)

   

2. $Contextualized\;Emotion\;Recognition\;in\;Conversation\;as\;Sequence\;Tagging$$\textcolor{Red}{(ACL,\;CCF-A)}$

   $\bullet\;Motivation:$

      作者发现关于ERC的预测情绪标签的方法大多会忽略情绪标签之间的内在联系，并且观察到情绪一致性现象广泛存在于对话中，也就是说，相似的情绪更有可能出现，而不是不同的情绪，所以认为对情绪一致性进行建模有助于找到更合理的情绪标签分布，从而进一步提高情绪分类的性能。

   $\bullet\;Method:$

      作者首次将ERC任务建模为序列标记，并使用CRF来建模对话中的情绪一致性，CRF层利用上文和下文的情感标签来联合解码整个对话的最佳标签序列。应用一个多层Transformer编码器来增强基于LSTM的全局上下文编码器，因为增强的编码器能够捕获长程连续上下文。作者提出了Contextualized Emotion Sequence Tagging（CESTa）模型，首先通过CNN网络获取每个句子的表示 ut ，随后每个句子分别被送入全局编码器（Global Context Encoder）和独立编码器（Individual Context Encoder)，最后全局编码器的结果和局部编码器的结果拼接，经过全连接层送入CRF层产生最终的预测，并选择最大分数的序列作为输出。

   $\bullet\;Experimental\;Result:$
   
      本文在三个对话数据集上做了实验，与baseline相比，本文的模型在三个数据集上均取得了SOTA结果，实验表明对情感一致性和远程上下文依赖关系进行建模可以提高情感分类的性能。**(Written by Yuzhao Chen)**   [(BibTex)](https://aclanthology.org/2020.sigdial-1.23/)

 

2021:

# Recognizing emotion cause

2016：

2017：

2018：

2019：

2020：

2021：

# Commonsense knowledge

2016：

2017：

2018：

2019：

2020：

2021：

# Self-supervised Learning in conversation

2016:

2017:

2018:

2019:

2020:

2021:

# Affective Computing

2016:

2017:

2018:

2019:

2020:

2021:

# Others (NLP)

2016:

2017:

2018:

2019:

2020:

2021:
