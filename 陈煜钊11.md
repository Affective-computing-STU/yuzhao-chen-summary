# Emotion recognition in conversation(ERC)

2016:

2017:

2018

2019:

2020:

2021:

# Recognizing emotion cause

2016：

2017：

2018：

2019：

2020：

2021：



# Commonsense knowledge

2016：

2017：

2018：

2019：

2020：

2021：

# Self-supervised Learning in conversation

2016:

2017:

2018:

2019:

2020:

2021:

# Affective Computing

2016:

2017:

2018:

2019:

2020:

2021:

# Others (NLP)

2016:

2017:

2018:

2019:

2020:

2021:

1. $Entity\;Structure\;Within\;and\;Throughout:\;Modeling\;Mention\;Dependencies\;for\;Document-Level\;Relation\;Extraction$$\textcolor{Red}{(AAAI,\;CCF-A)}$

   $\bullet\;Motivation:$

      实体作为关系提取任务中的基本要素，表现出一定的结构。

   $\bullet\;Method:$

      本文创新性地提出了实体结构（Entity Structure）这一概念，以依赖（dependency）的形式，对实体提及在文档中的分布进行定义，并设计了结构化自注意力网络（SSAN）在上下文编码的同时对实体结构进行建模。本文在每个自我注意构建块中设计了两个可选的转换模块，以产生注意偏差，从而自适应地规范其注意流。

   $\bullet\;Experimental\;Result:$

      实验表明，SSAN能够有效地在深度网络中引入实体结构的先验，指导注意力机制的传播，以增强模型对实体间交互关系的推理能力。SSAN在包括DocRED在内的多个常用文档级关系抽取任务上取得了当前最优效果。**(Written by Yuzhao Chen)**  [(BibTex)](https://arxiv.org/abs/2102.10249)

   

2. $ERNIE-ViL:\;Knowledge\;Enhanced\;Vision-Language\;Representations\;through\;Scene\;Graphs$$\textcolor{Red}{(AAAI,\;CCF-A)}$

   $\bullet\;Motivation:$

      视觉-语言预训练的目标是通过对齐语料学习多模态的通用联合表示，将各个模态之间的语义对齐信号融合到联合表示中，从而提升下游任务效果。已有的视觉语言预训练方法在预训练过程中没有区分普通词和语义词，学到的联合表示无法刻画模态间细粒度语义的对齐，如场景中物体、物体属性、物体间关系这些深度理解场景所必备的细粒度语义。

   $\bullet\;Method:$

      本文提出了知识增强的视觉-语言预训练技术ERNIE-ViL，将包含细粒度语义信息的场景图先验知识融入预训练过程，创建了物体预测、属性预测、关系预测三个预训练任务，在预训练过程中更加关注细粒度语义的跨模态对齐，从而学习到能够刻画更好跨模态语义对齐信息的联合表示。

   $\bullet\;Experimental\;Result:$

      ERNIE-ViL在视觉问答、视觉常识推理、引用表达式理解、跨模态文本检索、跨模态图像检索等5个多模态典型任务上取得了SOTA效果，同时，在视觉常识推理VCR榜单上取得第一。**(Written by Yuzhao Chen)**  [(BibTex)](https://www.aaai.org/AAAI21Papers/AAAI-6208.YuFei.pdf)

   

3. $MVFNet:\;Multi-View\;Fusion\;Network\;for\;Efficient\;Video\;Recognition$$\textcolor{Red}{(AAAI,\;CCF-A)}$

   $\bullet\;Motivation:$

      视频识别作为视频理解的基础技术，是近几年非常热门的计算机视觉研究方向。现有的基于3D卷积网络的方法识别精度优异但计算量偏大，基于2D网络的方法虽然相对轻量但精度不及3D卷积网络。

   $\bullet\;Method:$

      本文提出一种轻量的多视角融合模块用于高效率且高性能的视频识别，该模块是一个即插即用的模块，能够直接插入到现有的2D卷积网络中构成一个简单有效的模型，称为MVFNet。此外，MVFNet可以视为一种通用的视频建模框架，通过设置模块内的参数，MVFNet可转化为经典的C2D，SlowOnly和TSM网络。

   $\bullet\;Experimental\;Result:$

      实验结果显示，在五个视频benchmark上，MVFNet仅仅使用2D卷积网络的计算量就能够取得与当前最先进的3D卷积网络媲美甚至更高的性能。**(Written by Yuzhao Chen)**  [(BibTex)](https://www.aaai.org/AAAI21Papers/AAAI-1523.WuW.pdf)

​       
