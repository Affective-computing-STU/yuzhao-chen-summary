# Others (NLP)

2016:

2017:

2018:

2019:

1. $TENER:\;Adapting\;Transformer\;Encoder\;for\;Named\;Entity\;Recognition$$\textcolor{Red}{(ACL,\;CCF-A)}$

   $\bullet\;Motivation:$

     目前命名实体识别（NER）任务主要还是采用BiLSTM模型。虽然Transformer模型在其它NLP任务中取得了很大成功，并且在并行性和长程依赖方面有巨大优势，但其在NER中表现不佳。

   $\bullet\;Method:$

     作者分析了Transformer的注意力机制，发现其在方向性、相对位置、稀疏性方面不太适合NER任务。通过对注意力打分函数的简单改进，使得Transformer结构在NER任务上性能大幅提升。

   $\bullet\;Experimental\;Result:$

     作者在2个英文NER数据集和4个中文NER数据集都取得了优于BiLSTM的F1值。**(Written by Yuzhao Chen)**[(BibTex)](https://arxiv.org/abs/1911.04474)


2020:

1. $FLAT:\;Chinese\;NER\;Using\;Flat-Lattice\;Transformer$$\textcolor{Red}{(ACL,\;CCF-A)}$

   $\bullet\;Motivation:$

     近年来，字符-词的lattice结构被证明了用在中文命名实体识别上有很好的效果。然而，因为lattice结构的复杂性和动态性，现有的基于lattice的方法并不能很好的利用GPU的并行计算。

   $\bullet\;Method:$

     作者提出了FLAT: Flat-LAttice Transformer模型用于中文的NER任务，将lattice结构转化为包含数个span的平层结构。每个span对应一个字符或者潜在的词和它们在原lattice中的位置。通过对位置编码的精心设计和transformer的威力，FLAT可以很好的利用lattice信息，并拥有优秀的并行计算能力。

   $\bullet\;Experimental\;Result:$

     在四个实验数据集上的实验，表明了FLAT比其它的lexicon-based方法有更好的性能和效果。**(Written by Yuzhao Chen)**[(BibTex)](https://arxiv.org/pdf/2004.11795.pdf)

2021:

2022：

1. $Unified\;Named\;Entity\;Recognition\;as\;Word-Word\;Relation\;Classification$$\textcolor{Red}{(AAAI,\;CCF-A)}$

   $\bullet\;Motivation:$

     到目前为止，命名实体识别（NER）主要涉及三种类型，包括扁平、重叠（又称嵌套）和不连续的NER，这些类型大多是单独研究的。最近，人们对统一NER越来越感兴趣，用一个模型同时处理上述三个工作。目前表现最好的方法主要包括基于跨度的模型和序列到序列的模型，但是前者只关注边界识别，而后者可能会受到曝光偏差的影响。

   $\bullet\;Method:$

     作者提出了一种新的替代方法，通过将统一的NER建模为词-词关系分类，即W2NER。该体系结构通过有效地建模实体词与下一个相邻词和尾部词关系之间的相邻关系，解决了统一NER的核心瓶颈。基于W2NER方案，开发了一个神经框架，其中统一的NER被建模为二维单词对网格。然后，还提出多粒度二维卷积来更好地细化网格表示。最后，使用一个协预测器对单词关系进行充分推理。

   $\bullet\;Experimental\;Result:$

     作者对14个广泛使用的基准数据集（8个英文数据集和6个中文数据集）进行了广泛的实验，其中该模型超过了所有当前性能最好的基线，推动了统一NER的最先进性能。**(Written by Yuzhao Chen)**[(BibTex)](https://arxiv.org/pdf/2112.10070.pdf)
