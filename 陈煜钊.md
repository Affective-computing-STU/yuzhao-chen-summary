**2018MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations\#ACL**

**motivation**：为了解决没有大规模的多模态多方的数据集的问题，该文提出了MELD数据集，该数据集是基于EmotionLines数据集的。

**method**：不同于EmotionLines数据集的纯文本模式，该数据集还增加了视频，音频进行参考以便更准确的标注，并在原有数据集的七种情绪标注基础上增加了积极，消极，中性三分类的情感标注。添加了限制以便过滤掉跨越场景情节的异常对话，删除了三个标记都不同的语料和对话。

**experimental result**：最终得出的综合Fleiss’ kappa score从原有数据集的0.34提高到了0.43。之后在该数据集的基础上选择了一些基准模型测试F-score。

**2017Context-Dependent Sentiment Analysis in User-Generated Videos#ACL**

**motivation**：本文作者指出了近几年提出的关于多模态情感分析的许多方法中存在着不考虑话语之间的关系和依存性、忽略视频中话语的顺序等问题。

**method**：从而提出了一个基于LSTM（长短期记忆）的模型，使得话语能够在同一个视频中从其周围环境中捕获上下文信息，从而帮助分类过程。作者将特征提取分成了两个部分：1、不考虑话语之间的上下文关系和依赖性，提取上下文无关的单模特征；2、使用了基于LSTM的递归神经网络(RNN)方案来考虑相互依赖性以及影响。每个视频的每个话语的softmax上使用分类交叉熵来进行训练，引入了Dropout的来避免过拟合，并使用两个不同的框架实现多模态融合。

**experimental result**：实验结果表明，该框架在三个基准数据集上的性能优于现有技术5-10%，并且对通用性具有很高的鲁棒性。

**2018Contextual Inter-modal Attention for Multi-modal Sentiment Analysis#EMNLP**

**motivation**：本文提出了一种基于递归神经网络的多模态多话语注意框架的情感预测方法，该方法能够利用了跨多种模式的贡献特性用于相邻话语的情感分析。

**method**：该框架为一系列话语提取多模态信息，并将其输入三个独立的双向门控循环单元(GRU)，进行时间步或话语之间共享的密集(完全连接的)操作，然后对密集层输出应用多模态注意。特别地，作者采用了双模态注意框架，其中注意函数被应用于两两模态的表示，即视觉-文本、文本-听觉和听觉-视觉。最后，成对注意的输出和表示被连接并传递到softmax层进行分类。作者称之为多模态多话语双模态注意框架（MMMU-BA）。

**experimental result**：最终评价结果表明，该方法在MOSI和MOSEI数据集上的准确率分别为82.31%和79.80%。与最先进的数据集模型相比，性能提高了大约2和1个百分点。