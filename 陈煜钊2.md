# Emotion recognition in conversation(ERC)

*2016*:

*2017*:

*2018*:

1. $ICON\;Interactive\;Conversational\;Memory\;Network$$\textcolor{Red}{(EMNLP,\;CCF-B)}$

   *$\bullet\;Motivation:$*

      本文提出了交互式会话记忆网络(ICON)，这是一个多模态情绪检测框架，可以从会话视频中提取多模态特征，并将自我和其他说话者的情绪影响分层建模到全局记忆中。这些记忆产生了上下文总结，有助于预测话语视频的情绪倾向。

   $\bullet\;Method:$

      ICON被设计为对话情感建模的通用框架。它的计算可以分为四个连续的模块序列：多模态特征提取、自影响模块、动态全局影响模块和多跳记忆。多模态框架从语言（音频和视觉）三种模式中进行特征提取，自影响模块是由两个GRUs组成的，独立地处理两个说话者的历史，在建模上下文历史时，作者使用动态影响模块合并了这些属性。此模块维护会话的全局表示，并在每个k个长度的对话历史记录的时间步长时定期更新它，多跳记忆是为了获取GRU产生的记忆。ICON模型就是把两人的历史对话用同一层GRU连起来，另外就是每个记忆网络中都用GRU去建模下一次的M 输入。

   *$\bullet\;Experimental\;Result:$*

      本文分别给出了在IEMOCAP和SEMAINE测试集的结果，结果显示，ICON的表现优于比较的模型，在情绪方面表现有着显著提高。对于每种情绪，ICON的表现除了幸福情感均优于所有的比较模型。然而，其性能仍然与cLSTM相当，没有一个显著的差距。**(Written by Yuzhao Chen)**  [(BibTex)](https://aclanthology.org/D18-1280.pdf)

   

*2019:*

1. $DialogueRNN:\;An\;Attentive\;RNN\;for\;Emotion\;Detection\;in\;Conversations$ $\textcolor{Red}{(AAAI,\;CCF-A)}$

   *$\bullet\;Motivation:$*

      当前的系统，都无法以有意义的方式区分对话中的不同参与者。他们不知道某一特定话语的说话人。所以作者在本文描述了一种新的基    于递归神经网络（RNN）的方法，它在整个会话过程中跟踪当事人的状态，并将这些信息用于情感分类。

   *$\bullet\;Method:$*

      该模型基于以下假设：对话中存在与情感相关的三个主要方面：说话人，前几句话的语境，以及前几句话的情感。该系统采用三个GRU对这三个方面进行建模。传入的话语被馈入两个GRU，分别称为全局(globalGRU)和当事人(party GRU)以更新上下文和当事人状态。全局GRU在对话语进行编码的同时对相应的一方当事人信息进行编码。对这个GRU利用注意力，就会得到上下文表示，其中包含对话中不同参与方之前所有话语的信息。说话人的状态通过注意力和说话人之前的状态依赖于这个上下文。这样可以确保在时间t时，发言者状态直接从发言者的先前状态和全局GRU中获取信息，而全局GRU包含有关先前各方的信息。最后，将更新后的说话人状态输入到情感(emotion)GRU中，对给定话语的情感表示进行解码，该情感表示用于情感分类。在时刻t，emotion GRU单元得到t − 1时刻的情绪表示和t时刻的说话人状态。另一方面，party GRU对同一个当事人的两个顺序状态之间的关系进行建模。在DialogueRNN中，这三种不同类型的GRU都以一种循环的方式连接在一起。

   *$\bullet\;Experimental\;Result:$*

      将DialogueRNN与CMN模型在两个数据集上进行了文本模态的比较。IEMOCAP上，该模型平均以2.77％的准确性和3.76％的f1评分优于CMN。并且在6个情感类中的5个明显优于CMN。在AVEC上，该模型在valence,arousal,expectancy,power属性方面优于CMN；它产生的平均绝对误差（MAE）明显较低，而皮尔逊相关系数（r）则较高。作者认为这是由于当事人的状态和情感GRU的结合，这是CMN中缺失的。**(Written by Yuzhao Chen)**  [(BibTex)](https://ojs.aaai.org/index.php/AAAI/article/view/4657)

   

2. $DialogueGCN:\;A\;Graph\;Convolutional\;Neural\;Network\;for\;Emotion\;Recognition\;in\;Conversation$$\textcolor{Red}{(EMNLP,\;CCF-B)}$

   $\bullet\;Motivation:$

      本文提出了对话图卷积网络(Dialogue Graph Convolutional Network, Dialogue)，一种基于图神经网络的ERC方法。作者利用对话者的自我依赖和说话者之间的依赖来建立情绪识别的会话语境模型。通过图网络，DialogueGCN解决了当前基于rnn的方法中存在的上下文传播问题。

   *$\bullet\;Method:$*

      该文引入了inter-dependency和intra-dependency的概念，inter-dependency就是说话者受到外界话语的影响，而intra-dependency是说话者受自身之前的话语的影响。主要的模型分为三个部分，第一个是Sequential Context Encoder部分，将说话者的话语在序列层面encoder成向量。第二个是个Speaker-Level Context Encoder部分，就是分说话者进行encoding，也是建立所谓的inter-dependency和intra-dependency的时候，实质是通过GCN的传播找到各句话之间的联系。第三个部分就是一个Emotion Classify层。

   *$\bullet\;Experimental\;Result:$*

      在 IEMOCAP 的 6 个类别中的 4 个类别，相对于包括 DialogueRNN 在内的所有模型，DialogueGCN 均显示出明显的改进。DialogueGCN 在 AVEC 和 MELD 上产生了相似的结果，超过了现有的 DialogueRNN。作者还进行了消融研究：一次取下一个编码器，然后重新测量模型的性能，发现说话者级上下文编码器比顺序上下文编码器更重要，因此，将说话者级别的上下文添加到对话图中，这种方式可以从本质上提高模型的理解能力。由此可见，DialogueRNN 能够很好地捕获顺序上下文，但是缺乏对说话人上下文进行编码的能力。**(Written by Yuzhao Chen)**  [(BibTex)](https://arxiv.org/abs/1908.11540)

*2020*:

*2021*:



